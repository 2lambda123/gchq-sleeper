# A string to uniquely identify this deployment. This should be no longer than 20 chars.
# It should be globally unique as it will be used to name AWS resources such as S3 buckets.
sleeper.id=full-example

# A comma separated list of paths containing the table properties files. These can either be paths to
# the properties files themselves or paths to directories which contain the table properties.
sleeper.table.properties=example/full/table.properties,/path/to/my/folder

# The S3 bucket containing the jar files of the Sleeper components.
sleeper.jars.bucket=<the name of the bucket containing your jars, e.g. sleeper-<insert-unique-name-here>-jars

# A comma-separated list of the jars containing application specific iterator code.
# These jars are assumed to be in the bucket given by sleeper.jars.bucket, e.g. if that
# bucket contains two iterator jars called iterator1.jar and iterator2.jar then the
# property should be sleeper.userjars=iterator1.jar,iterator2.jar
sleeper.userjars=

# A file of key-value tags. These will be added to all the resources in this deployment.
sleeper.tags.file=example/full/tags.properties

# Whether to keep the sleeper table bucket, Dynamo tables, query results bucket, etc., when the
# instance is destroyed.
sleeper.retain.infra.after.destroy=true

# The optional stacks to deploy.
sleeper.optional.stacks=CompactionStack,GarbageCollectorStack,IngestStack,PartitionSplittingStack,QueryStack,AthenaStack,EmrBulkImportStack,DashboardStack

# The AWS account number. This is the AWS account that the instance will be deployed to.
sleeper.account=1234567890

# The AWS region to deploy to.
sleeper.region=eu-west-2

# The version of Sleeper to use. This property is used to identify the correct jars in the S3JarsBucket and to
# select the correct tag in the ECR repositories.
sleeper.version=0.11.0

# The id of the VPC to deploy to.
sleeper.vpc=1234567890

# Whether to check that the VPC that the instance is deployed to has an S3 endpoint. If there
# is no S3 endpoint then the NAT costs can be very significant.
sleeper.vpc.endpoint.check=true

# The subnet to deploy ECS tasks to.
sleeper.subnet=subnet-abcdefgh

# The Hadoop filesystem used to connect to S3.
sleeper.filesystem=s3a://

# An email address used by the TopicStack to publish SNS notifications of errors.
sleeper.errors.email=

# The visibility timeout on the queues used in compactions, partition splitting, etc.
sleeper.queue.visibility.timeout.seconds=900

# The length of time in days that CloudWatch logs from lambda functions, ECS containers, etc., are retained.
# See https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-logs-loggroup.html for valid options.
# Use -1 to indicate infinite retention.
sleeper.log.retention.days=30

# Used to set the value of fs.s3a.connection.maximum on the Hadoop configuration. This controls the
# maximum number of http connections to S3. See https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/performance.html
sleeper.s3.max-connections=25

# The version of Fargate to use.
sleeper.fargate.version=1.4.0

# The amount of memory for the lambda that creates ECS tasks to execute compaction and ingest jobs.
sleeper.task.runner.memory=1024

# The timeout in seconds for the lambda that creates ECS tasks to execute compaction jobs and ingest jobs.
# This must be >0 and <= 900.
sleeper.task.runner.timeout.seconds=900

# The namespaces for the metrics used in the metrics stack.
sleeper.metrics.namespace=Sleeper


## The following properties relate to standard ingest.

# The name of the ECR repository for the ingest container. The Docker image from the ingest module should have been
# uploaded to an ECR repository of this name in this account.
sleeper.ingest.repo=<insert-unique-sleeper-id>/ingest

# The maximum number of concurrent ECS tasks to run.
sleeper.ingest.max.concurrent.tasks=200

# The frequency in minutes with which an EventBridge rule runs to trigger a lambda that, if necessary, runs more ECS
# tasks to perform ingest jobs.
sleeper.ingest.task.creation.period.minutes=1

# The frequency, in seconds, with which change message visibility requests are sent to extend the
# visibility of messages on the ingest queue so that they are not processed by other processes.
# This should be less than the value of sleeper.queue.visibility.timeout.seconds.
sleeper.ingest.keepalive.period.seconds=300

# This sets the value of fs.s3a.experimental.input.fadvise on the Hadoop configuration used to read and write
# files to and from S3 in ingest jobs. Changing this value allows you to fine-tune how files are read. Possible
# values are "normal", "sequential" and "random". More information is available here:
# https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/performance.html#fadvise
sleeper.ingest.fs.s3a.experimental.input.fadvise=sequential

# The amount of CPU and memory used by Fargate tasks that perform ingest jobs. Note that only certain combinations
# are valid (see https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-cpu-memory-error.html).
sleeper.ingest.task.cpu=2048
sleeper.ingest.task.memory=4096

# The frequeney in seconds with which ingest tasks refresh their view of the partitions.
# (NB Refreshes only happen once a batch of data has been written so this is a lower bound
# on the refresh frequency.)
sleeper.ingest.partition.refresh.period=120

# The way in which records are held in memory before they are written to a local store.
# Valid values are 'arraylist' and 'arrow'.
# The arraylist method is simpler, but it is slower and requires careful tuning of the number of records in each batch.
# Note that the arrow approach does not currently support schemas containing lists or maps.
sleeper.ingest.record.batch.type=arraylist

# The way in which partition files are written to the main Sleeper store
# Valid values are 'direct' (which writes using the s3a Hadoop file system) and 'async' (which writes locally and then
# copies the completed Parquet file asynchronously into S3).
# The direct method is simpler but the async method should provide better performance when the number of partitions
# is large.
sleeper.ingest.partition.file.writer.type=direct

# The maximum number of records written to local file in an ingest job. (Records are written in sorted order to local
# disk before being uploaded to S3. Increasing this value increases the amount of time before data is visible in the
# system, but increases the number of records written to S3 in a batch, therefore reducing costs.)
# (arraylist-based ingest only)
sleeper.ingest.max.local.records=100000000

# The maximum number of records to read into memory in an ingest job. (Up to sleeper.ingest.memory.max.batch.size
# records are read into memory before being sorted and written to disk. This process is repeated until
# sleeper.ingest.max.local.records records have been written to local files. Then the sorted files and merged and
# the data is written to sorted files in S3.)
# (arraylist-based ingest only)
sleeper.ingest.memory.max.batch.size=1000000

# The number of bytes to allocate to the Arrow working buffer. This buffer is used for sorting and other sundry
# activities.
# Note that this is off-heap memory, which is in addition to the memory assigned to the JVM.
# (arrow-based ingest only) [128MB]
sleeper.ingest.arrow.working.buffer.bytes=134217728

# The number of bytes to allocate to the Arrow batch buffer, which is used to hold the records before they are
# written to local disk. A larger value means that the local disk holds fewer, larger files, which are more efficient
# to merge together during an upload to S3. Larger values may require a larger working buffer.
# Note that this is off-heap memory, which is in addition to the memory assigned to the JVM.
# (arrow-based ingest only) [1GB]
sleeper.ingest.arrow.batch.buffer.bytes=1073741824

# The maximum number of bytes to store on the local disk before uploading to the main Sleeper store. A larger value
# reduces the number of S3 PUTs that are required to upload thle data to S3 and results in fewer files per partition.
# (arrow-based ingest only) [16GB]
sleeper.ingest.arrow.max.local.store.bytes=17179869184

# The number of bytes to write at once into an Arrow file in the local store. A single Arrow file contains many of
# these micro-batches and so this parameter does not significantly affect the final size of the Arrow file.
# Larger values may require a larger working buffer. There should be no need to tune this parameter.
# (arrow-based ingest only) [64K]
sleeper.ingest.arrow.max.single.write.to.file.bytes=65536

# The name of a bucket that contains files to be ingested via ingest jobs. This bucket should already
# exist, i.e. it will not be created as part of the cdk deployment of this instance of Sleeper. The ingest
# and bulk import stacks will be given read access to this bucket so that they can consume data from it.
sleeper.ingest.source.bucket=


## The following properties relate to bulk import, i.e. ingesting data using Spark jobs running on EMR or EKS.

# In the Dataframe-based Spark bulk import job, the data is normally coalesced into the same number of partitions
# as there are leaf partitions in the table. This can cause problems if there are only a small number of partitions
# in the table. This property sets the minimum number of partitions for this coalesce to happen, e.g. if there
# are fewer than 100 leaf partitions then the coalesce will not happen.
sleeper.bulk.import.min.partitions.coalesce=100

# (EMR mode only) The default EMR release label to be used when creating an EMR cluster for bulk importing data
# using Spark running on EMR. This default can be overridden by a table property or by a property in the
# bulk import job specification.
sleeper.default.bulk.import.emr.release.label=emr-6.4.0

# (EMR mode only) The default EC2 instance type to be used for the master node of the EMR cluster. This default can
# be overridden by a table property or by a property in the bulk import job specification.
sleeper.default.bulk.import.emr.master.instance.type=m5.xlarge

# (EMR mode only) The default EC2 instance type to be used for the executor nodes of the EMR cluster. This default can
# be overridden by a table property or by a property in the bulk import job specification.
sleeper.default.bulk.import.emr.executor.instance.type=m5.4xlarge

# (EMR mode only) The default initial number of EC2 instances to be used as executors in the EMR cluster. This default can
# be overridden by a table property or by a property in the bulk import job specification.
sleeper.default.bulk.import.emr.executor.initial.instances=2

# (EMR mode only) The default maximum number of EC2 instances to be used as executors in the EMR cluster. This default can
# be overridden by a table property or by a property in the bulk import job specification.
sleeper.default.bulk.import.emr.executor.max.instances=10

# (EMR mode only) The name of a bucket created for use with bulk import mechanism. This can be used to store
# logs or read data from. If you do not specify a name but set "sleeper.bulk.import.emr.bucket.create" to true,
# a name will be auto-generated.
sleeper.bulk.import.emr.bucket=

# (EMR mode only) Whether the CDK deployment should create a bucket to store logs and data. If set to false, and no bucket is
# specified via "sleeper.bulk.import.bucket", no bucket is created. If set to false but "sleeper.bulk.import.bucket"
# is set, it will be assumed that the bucket already exists. If set to true, a bucket will be created,
# Either with the provided name or an auto-generated name
sleeper.bulk.import.emr.bucket.create=true

# (EMR mode only) An EC2 keypair to use for the EC2 instances. Specifying this will allow you to SSH to the nodes
# in the cluster while it's running.
sleeper.bulk.import.keypair.name=my-key

# (Persistent EMR mode only) Specifying this security group causes the group to be added to the EMR master's
# list of security groups.
sleeper.bulk.import.emr.master.additional.security.group=

# (Persistent EMR mode only) The EMR release used to create the persistent EMR cluster.
sleeper.bulk.import.persistent.emr.release.label=emr-6.4.0

# (Persistent EMR mode only) The EC2 instance type used for the master of the persistent EMR cluster.
sleeper.bulk.import.persistent.emr.master.instance.type=m5.xlarge

# (Persistent EMR mode only) The EC2 instance type used for the executor nodes of the persistent EMR cluster.
sleeper.bulk.import.persistent.emr.core.instance.type=m5.4xlarge

# (Persistent EMR mode only) Whether the persistent EMR cluster should use managed scaling or not.
sleeper.bulk.import.persistent.emr.use.managed.scaling=true

# (Persistent EMR mode only) The minimum number of instances in the persistent EMR cluster. If managed
# scaling is not used then the cluster will be of fixed size, with number of instances equal to this
# value.
sleeper.bulk.import.persistent.emr.min.instances=1

# (Persistent EMR mode only) The maximum number of instances in the persistent EMR cluster. This value
# is only used if managed scaling is not used.
sleeper.bulk.import.persistent.emr.max.instances=10

# (EKS mode only) The name of the ECS repository where the Docker image for the bulk import container is stored.
sleeper.bulk.import.repo=<insert-unique-sleeper-id>//bulk-import-runner

# This is used to set the value of spark.shuffle.mapStatus.compression.codec on the Spark configuration
# for bulk import jobs. Setting this to "lz4" stops "Decompression error: Version not supported" errors
# - only a value of "lz4" has been tested. This default can be overridden by a table property or by a
# property in the bulk import job specification.
sleeper.default.bulk.import.spark.shuffle.mapStatus.compression.codec=lz4

# This is used to set the value of spark.speculation on the Spark configuration for bulk import jobs.
# This default can be overridden by a table property or by a property in the bulk import job specification.
sleeper.default.bulk.import.spark.speculation=false

# This is used to set the value of spark.speculation.quantile on the Spark configuration. Lowering this
# from the default 0.75 allows us to try re-running hanging tasks sooner. This default can be overridden
# by a table property or by a property in the bulk import job specification.
sleeper.default.bulk.import.spark.speculation.quantile=0.75


## The following properties relate to the splitting of partitions.

# The frequency in minutes with which the lambda that finds partitions that need splitting runs.
sleeper.partition.splitting.period.minutes=60

# When a partition needs splitting, a partition splitting job is created. This reads in the sketch files
# associated to the files in the partition in order to identify the median. This parameter controls the
# maximum number of files that are read in.
sleeper.partition.splitting.files.maximum=50

# The amount of memory in MB for the lambda function used to identify partitions that need to be split.
sleeper.partition.splitting.finder.memory=2048

# The timeout in seconds for the lambda function used to identify partitions that need to be split.
sleeper.partition.splitting.finder.timeout.seconds=900

# The memory for the lambda function used to split partitions.
sleeper.partition.splitting.memory=2048

# The timeout in seconds for the lambda function used to split partitions.
sleeper.partition.splitting.timeout.seconds=900

# This is the default value of the partition splitting threshold. Partitions with more than the following
# number of records in will be split. This value can be overridden on a per-table basis.
sleeper.default.partition.splitting.threshold=1000000000


## The following properties relate to garbage collection.

# The frequency in minutes with which the garbage collector lambda is run.
sleeper.gc.period.minutes=15

# The memory in MB for the lambda function used to perform garbage collection.
sleeper.gc.memory=1024

# The size of the batch of files ready for garbage collection requested from the State Store.
sleeper.gc.batch.size=2000

# A file will not be deleted until this number of seconds have passed after it has been marked as ready for
# garbage collection. The reason for not deleting files immediately after they have been marked as ready for
# garbage collection is that they may still be in use by queries. This property can be overridden on a per-table
# basis.
sleeper.default.gc.delay.seconds=600


## The following properties relate to compactions.

# The name of the repository for the compaction container. The Docker image from the compaction-job-execution module
# should have been uploaded to an ECR repository of this name in this account.
sleeper.compaction.repo=<insert-unique-sleeper-id>/compaction-job-execution

# The visibility timeout for the queue of compaction jobs.
sleeper.compaction.queue.visibility.timeout.seconds=900

# The frequency, in seconds, with which change message visibility requests are sent to extend the
# visibility of messages on the compaction job queue so that they are not processed by other processes.
# This should be less than the value of sleeper.queue.visibility.timeout.seconds.
sleeper.compaction.keepalive.period.seconds=300

# The rate at which the compaction job creation lambda runs (in minutes, must be >=1).
sleeper.compaction.job.creation.period.minutes=1

# The amount of memory for the lambda that creates compaction jobs.
sleeper.compaction.job.creation.memory=1024

# The timeout for the lambda that creates compaction jobs in seconds.
sleeper.compaction.job.creation.timeout.seconds=900

# The maximum number of concurrent compaction tasks to run.
sleeper.compaction.max.concurrent.tasks=300

# The rate at which a check to see if compaction ECS tasks need to be created is made (in minutes, must be >= 1).
sleeper.compaction.task.creation.period.minutes=1
	
# The cpu and memory for a compaction task.
# See https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-cpu-memory-error.html for valid options.
sleeper.compaction.task.cpu=2048
sleeper.compaction.task.memory=4096

# The name of the class that defines how compaction jobs should be created.
# This should implement sleeper.compaction.strategy.CompactionStrategy. The value of this property is the
# default value which can be overridden on a per-table basis.
sleeper.default.compaction.strategy.class=sleeper.compaction.strategy.impl.SizeRatioCompactionStrategy

# The minimum number of files to read in a compaction job. Note that the state store
# must support atomic updates for this many files. For the DynamoDBStateStore this
# is 11. It can be overridden on a per-table basis.
# (NB This does not apply to splitting jobs which will run even if there is only 1 file.)
# This is a default value and will be used if not specified in the table.properties file
sleeper.default.compaction.files.batch.size=11

# Used by the SizeRatioCompactionStrategy to decide if a group of files should be compacted.
# If the file sizes are s_1, ..., s_n then the files are compacted if s_1 + ... + s_{n-1} >= ratio * s_n.
# It can be overridden on a per-table basis.
sleeper.default.table.compaction.strategy.sizeratio.ratio=3

# Used by the SizeRatioCompactionStrategy to control the maximum number of jobs that can be running
# concurrently per partition. It can be overridden on a per-table basis.
sleeper.default.table.compaction.strategy.sizeratio.max.concurrent.jobs.per.partition=100000


## The following properties relate to queries.

# The maximum number of simultaneous connections to S3 from a single query runner. This is separated
# from the main one as it's common for a query runner to need to open more files at once.
sleeper.query.s3.max-connections=1024

# The amount of memory in MB for the lambda that executes queries.
sleeper.query.processor.memory=2048

# The timeout for the lambda that executes queries in seconds.
sleeper.query.processor.timeout.seconds=900

# The frequency with which the query processing lambda refreshes its knowledge of the system state
# (i.e. the partitions and the mapping from partition to files), in seconds.
sleeper.query.processor.state.refresh.period.seconds=60

# The maximum number of records to include in a batch of query results send to
# the results queue from the query processing lambda.
sleeper.query.processor.results.batch.size=2000

# This value is used to set the time-to-live on the tracking of the queries in the DynamoDB-based query tracker.
sleeper.query.tracker.ttl.days=1

# The length of time the results of queries remain in the query results bucket before being deleted.
sleeper.query.results.bucket.expiry.days=7

# The default value of the rowgroup size used when the results of queries are written to Parquet files. The
# value given below is 8MiB. This value can be overridden using the query config.
sleeper.default.query.results.rowgroup.size=8388608

# The default value of the page size used when the results of queries are written to Parquet files. The
# value given below is 128KiB. This value can be overridden using the query config.
sleeper.default.query.results.page.size=131072


## The following properties relate to the dashboard.

# The period in minutes used in the dashboard.
sleeper.dashboard.time.window.minutes=5


## The following properties relate to logging.

# Logging Levels. These set the logging levels for different classes throughout the app. The list of allowed values
# are (from most verbose to least): TRACE, DEBUG, INFO (default), WARN, ERROR, FATAL. If you want more control than
# this, you can edit the log4j.properties file in core/src/main/resources

# For logging Sleeper classes. This does not apply to the MetricsLogger which is always set to INFO.
sleeper.logging.level=INFO

# For Apache logs that are not Parquet.
sleeper.logging.apache.level=INFO

# For Parquet logs.
sleeper.logging.parquet.level=WARN

# For AWS logs.
sleeper.logging.aws.level=INFO

# For everything else.
sleeper.logging.root.level=INFO


## The following properties relate to the integration with Athena.

# The number of days before objects in the spill bucket are deleted.
sleeper.athena.spill.bucket.ageoff.days=1

# The fully qualified composite classes to deploy. These are the classes that interact with Athena.
# You can choose to remove one if you don't need them. Both are deployed by default.
sleeper.athena.handler.classes=sleeper.athena.composite.SimpleCompositeHandler,sleeper.athena.composite.IteratorApplyingHandler

# The amount of memory (GB) the athena composite handler has
sleeper.athena.handler.memory=4096

# The timeout in seconds for the athena composite handler
sleeper.athena.handler.timeout.seconds=900


## The following properties are default values.

# The readahead range set on the Hadoop configuration when reading Parquet files in a query
# (see https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html).
sleeper.default.fs.s3a.readahead.range=64K

# The size of the row group in the Parquet files (default is 8MiB).
sleeper.default.rowgroup.size=8388608

# The size of the pages in the Parquet files (default is 128KiB).
sleeper.default.page.size=131072

# The compression codec to use in the Parquet files
sleeper.default.compression.codec=ZSTD

# This specifies whether point in time recovery is turned on for DynamoDB tables. This default can
# be overridden by a table property.
sleeper.default.table.dynamo.pointintimerecovery=false

# This specifies whether queries and scans against DynamoDB tables used in the DynamoDB state store
# are strongly consistent. This default can be overriden by a table property.
sleeper.default.table.dynamo.strongly.consistent.reads=false
